{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMwT+N5mx7rGiGvEUggNLz"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "4sbARl4y27hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Input directory where your stock datasets are stored\n",
        "input_directory = '/content/processed_files'\n",
        "\n",
        "# Initialize an empty list to store DataFrames\n",
        "dataframes = []\n",
        "\n",
        "# Loop through all CSV files in the directory\n",
        "for filename in os.listdir(input_directory):\n",
        "    if filename.endswith('.csv'):  # Ensure only CSV files are selected\n",
        "        file_path = os.path.join(input_directory, filename)\n",
        "        # Read the CSV file into a DataFrame\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "        # Append the DataFrame to the list\n",
        "        dataframes.append(df)\n",
        "\n",
        "# Concatenate all DataFrames into a single DataFrame\n",
        "final_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Display the concatenated DataFrame\n",
        "print(final_df.head())\n",
        "\n",
        "# Save the concatenated DataFrame to a new CSV file\n",
        "final_df.to_csv('/content/Stocks2.csv', index=False)\n",
        "print(\"All datasets combined and saved as 'all_stocks_combined.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFkdPZJqitUG",
        "outputId": "8fc647a2-0a32-4209-82ec-1a6fe42cf459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-c70e19b39a4c>:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  final_df = pd.concat(dataframes, ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     S.N. Symbol        Date    LTP   High    Low   Qty  Turnover  SMA_5  \\\n",
            "0  2377.0  SADBL  2012-09-25  147.0  152.0  147.0    30    4480.0  169.0   \n",
            "1  2376.0  SADBL  2012-09-26  143.0  145.0  143.0    70   10030.0  159.6   \n",
            "2  2375.0  SADBL  2012-09-27  137.0  141.0  137.0    60    8340.0  150.8   \n",
            "3  2374.0  SADBL  2012-10-02  124.0  135.0  124.0   200   25460.0  141.2   \n",
            "4  2373.0  SADBL  2012-10-03  112.0  122.0  112.0  4850  544740.0  132.6   \n",
            "\n",
            "    returns  ...       MACD  Volatility  Volume_MA5  Volume_MA20  month  \\\n",
            "0 -0.051613  ... -36.547462    0.025549     73404.0      64561.5      9   \n",
            "1 -0.027211  ... -37.781527    0.025759     56410.0      64906.0      9   \n",
            "2 -0.041958  ... -38.796462    0.025648     10294.0      64553.0      9   \n",
            "3 -0.094891  ... -40.186551    0.030081     11602.0      64214.5     10   \n",
            "4 -0.096774  ... -41.774950    0.032350    118610.0      88919.5     10   \n",
            "\n",
            "   day_of_week day_of_month week_of_year days_from_start High   \n",
            "0            1           25           39               0   NaN  \n",
            "1            2           26           39               1   NaN  \n",
            "2            3           27           39               2   NaN  \n",
            "3            1            2           40               7   NaN  \n",
            "4            2            3           40               8   NaN  \n",
            "\n",
            "[5 rows x 22 columns]\n",
            "All datasets combined and saved as 'all_stocks_combined.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where your CSV files are located\n",
        "input_directory = '/content/'\n",
        "# Directory where you want to save the processed files\n",
        "output_directory = '/content/processed_files/'\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_file(file_path):\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Convert numeric columns to proper format\n",
        "    numeric_columns = ['Qty', 'Turnover', 'LTP', 'High', 'Low']\n",
        "    for col in numeric_columns:\n",
        "        if col in df.columns:  # Check if the column exists\n",
        "            df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', ''), errors='coerce')\n",
        "\n",
        "    # Convert 'Date' column to datetime and clean the data\n",
        "    if 'Date' in df.columns:\n",
        "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "        df = df.sort_values(by='Date')\n",
        "        df = df.drop_duplicates(subset='Date', keep='first').reset_index(drop=True)\n",
        "\n",
        "    # Add technical indicators\n",
        "    if 'LTP' in df.columns and 'Turnover' in df.columns:\n",
        "        df['SMA_5'] = df['LTP'].rolling(window=5).mean()\n",
        "        df['returns'] = df['LTP'].pct_change()\n",
        "        df['SMA_20'] = df['LTP'].rolling(window=20).mean()\n",
        "        delta = df['LTP'].diff()\n",
        "        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "        rs = gain / loss\n",
        "        df['RSI'] = 100 - (100 / (1 + rs))\n",
        "        EMA_12 = df['LTP'].ewm(span=12, adjust=False).mean()\n",
        "        EMA_26 = df['LTP'].ewm(span=26, adjust=False).mean()\n",
        "        df['MACD'] = EMA_12 - EMA_26\n",
        "        df['Volatility'] = df['returns'].rolling(window=10).std()\n",
        "        df['Volume_MA5'] = df['Turnover'].rolling(window=5).mean()\n",
        "        df['Volume_MA20'] = df['Turnover'].rolling(window=20).mean()\n",
        "\n",
        "    # Drop rows with NaN values (optional)\n",
        "    df = df.dropna()\n",
        "\n",
        "    df['month'] = df['Date'].dt.month.astype(\"str\").astype('category')\n",
        "    df['day_of_week'] = df['Date'].dt.dayofweek.astype(\"str\").astype('category')\n",
        "    df['day_of_month'] = df['Date'].dt.day.astype(\"str\").astype('category')\n",
        "    df['week_of_year'] = df['Date'].dt.isocalendar().week\n",
        "    df['days_from_start'] = (df['Date'] - df['Date'].min()).dt.days\n",
        "\n",
        "\n",
        "    return df\n",
        "\n",
        "# Process each CSV file in the /content/ directory\n",
        "for filename in os.listdir(input_directory):\n",
        "    if filename.endswith('.csv'):  # Process only .csv files\n",
        "        input_path = os.path.join(input_directory, filename)\n",
        "        output_path = os.path.join(output_directory, filename)\n",
        "\n",
        "        # Preprocess the file\n",
        "        print(f\"Processing {filename}...\")\n",
        "        processed_df = preprocess_file(input_path)\n",
        "\n",
        "        # Save the processed file to the output directory\n",
        "        processed_df.to_csv(output_path, index=False)\n",
        "        print(f\"Saved processed file to {output_path}\")\n",
        "\n",
        "print(f\"All files have been processed and saved to {output_directory}\")"
      ],
      "metadata": {
        "id": "deTB0QR6i70x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0253853-0e5c-4294-d9ef-f85fe6674f42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing LSL.csv...\n",
            "Saved processed file to /content/processed_files/LSL.csv\n",
            "Processing NABBC_stock_history.csv...\n",
            "Saved processed file to /content/processed_files/NABBC_stock_history.csv\n",
            "Processing FinalNIMB.csv...\n",
            "Saved processed file to /content/processed_files/FinalNIMB.csv\n",
            "Processing GRDBL.csv...\n",
            "Saved processed file to /content/processed_files/GRDBL.csv\n",
            "All files have been processed and saved to /content/processed_files/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l-yrEyhTcrgK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}